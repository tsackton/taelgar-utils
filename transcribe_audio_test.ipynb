{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import shutil\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_wav(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Converts an audio file of any supported format to WAV format using pydub, with error checking.\n",
    "    \n",
    "    :param input_file: Path to the input audio file (e.g., m4a, mp3, wav, ogg).\n",
    "    :param output_file: Optional, path to save the converted wav file. \n",
    "                        If not provided, the output file will have the same name as the input but with a .wav extension.\n",
    "    :return: Path to the converted WAV file or None if conversion fails.\n",
    "    \"\"\"\n",
    "    # Check if ffmpeg is installed and available\n",
    "    if not shutil.which(\"ffmpeg\"):\n",
    "        raise EnvironmentError(\"ffmpeg is not installed or not found in system PATH. Please install ffmpeg.\")\n",
    "\n",
    "    # Detect file format based on extension\n",
    "    file_format = input_file.split('.')[-1].lower()\n",
    "\n",
    "    # Supported formats by pydub + ffmpeg\n",
    "    supported_formats = ['m4a', 'mp3', 'ogg', 'flv', 'aac', 'wma', 'flac', 'wav']\n",
    "\n",
    "    # Check if the file format is supported\n",
    "    if file_format not in supported_formats:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}. Supported formats are: {supported_formats}\")\n",
    "\n",
    "    try:\n",
    "        # Try to load the audio file\n",
    "        audio = AudioSegment.from_file(input_file, format=file_format)\n",
    "\n",
    "        # If output file is not provided, generate the output file name\n",
    "        if output_file is None:\n",
    "            output_file = input_file.rsplit('.', 1)[0] + '.wav'\n",
    "\n",
    "        # Export the audio file as wav\n",
    "        audio.export(output_file, format=\"wav\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The input file '{input_file}' does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting the audio file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_diarization(wav_file, hf_token):\n",
    "    \"\"\"\n",
    "    Perform speaker diarization on a WAV audio file using pyannote.audio, with Hugging Face token for authentication.\n",
    "    Utilizes MPS backend on Mac M1/M2 if available.\n",
    "    \n",
    "    :param wav_file: Path to the WAV file.\n",
    "    :param hf_token: Hugging Face token for accessing the pre-trained model.\n",
    "    :return: Diarization object with speaker segments and timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if MPS is available, otherwise default to CPU\n",
    "        device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        # Load the speaker diarization pipeline using the provided Hugging Face token\n",
    "        pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=hf_token)\n",
    "\n",
    "        # Perform speaker diarization on the specified device (MPS or CPU)\n",
    "        diarization = pipeline(wav_file)\n",
    "\n",
    "        # Collect the results in a structured format\n",
    "        results = []\n",
    "        for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            result = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": segment.start,\n",
    "                \"end_time\": segment.end\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"Speaker {speaker} speaks from {segment.start:.1f}s to {segment.end:.1f}s\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during diarization: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_by_size(input_file, max_size_mb=25, overlap_ms=1000):\n",
    "    \"\"\"\n",
    "    Splits an audio file into chunks small enough to send to the Whisper API, with some overlap to avoid cutting words.\n",
    "    \n",
    "    :param input_file: Path to the input WAV file.\n",
    "    :param max_size_mb: Maximum size of each audio chunk in MB (25 MB for Whisper API).\n",
    "    :param overlap_ms: Overlap duration between consecutive chunks in milliseconds.\n",
    "    :return: List of file paths for each audio chunk.\n",
    "    \"\"\"\n",
    "    # Load the audio file using pydub\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "    \n",
    "    # Estimate how many bytes are in one second of the audio\n",
    "    bytes_per_second = audio.frame_rate * audio.frame_width * audio.channels\n",
    "    \n",
    "    # Convert MB to bytes\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    \n",
    "    # Calculate the chunk duration in milliseconds (chunk_duration = max_size / bytes_per_second)\n",
    "    chunk_duration_ms = math.floor((max_size_bytes / bytes_per_second) * 1000)  # Convert seconds to milliseconds\n",
    "    \n",
    "    # Split the audio into chunks with overlap\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(audio):\n",
    "        end = start + chunk_duration_ms\n",
    "        chunks.append(audio[start:end])\n",
    "        start = end - overlap_ms  # Move to the next chunk with overlap\n",
    "    \n",
    "    # Save each chunk to a separate WAV file\n",
    "    chunk_files = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = f\"{os.path.splitext(input_file)[0]}_chunk_{i}.wav\"\n",
    "        chunk.export(chunk_filename, format=\"wav\")\n",
    "        chunk_files.append(chunk_filename)\n",
    "\n",
    "    return chunk_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_chunk(chunk_file, whisper_api_key):\n",
    "    \"\"\"\n",
    "    Transcribe a single audio chunk using Whisper API.\n",
    "    \n",
    "    :param chunk_file: Path to the audio chunk (WAV file).\n",
    "    :param whisper_api_key: OpenAI API key for Whisper API.\n",
    "    :return: Transcription text of the audio chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=whisper_api_key)\n",
    "\n",
    "    try:\n",
    "        # Open the chunk file\n",
    "        with open(chunk_file, \"rb\") as audio_file:\n",
    "            # Send the audio file to Whisper API for transcription\n",
    "            response = client.audio.transcriptions.create(\n",
    "                file=audio_file,\n",
    "                model=\"whisper-1\",\n",
    "            )\n",
    "            return response.text\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during transcription: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlap_from_transcriptions(transcriptions, overlap_word_count=5):\n",
    "    \"\"\"\n",
    "    Remove overlapping parts of transcriptions by detecting repeated words at the boundary of consecutive chunks.\n",
    "    \n",
    "    :param transcriptions: List of transcribed text for each chunk.\n",
    "    :param overlap_word_count: Number of words to compare at the end of one chunk and the start of the next.\n",
    "    :return: Cleaned and combined transcription.\n",
    "    \"\"\"\n",
    "    final_transcription = []\n",
    "    \n",
    "    for i in range(len(transcriptions)):\n",
    "        # Add the first chunk directly\n",
    "        if i == 0:\n",
    "            final_transcription.append(transcriptions[i])\n",
    "        else:\n",
    "            # Compare the last few words of the previous chunk with the start of the current chunk\n",
    "            previous_chunk = final_transcription[-1].split()\n",
    "            current_chunk = transcriptions[i].split()\n",
    "            \n",
    "            # Find overlap by comparing the last few words of the previous chunk and the first few words of the current chunk\n",
    "            overlap_start = 0\n",
    "            for j in range(min(overlap_word_count, len(previous_chunk), len(current_chunk))):\n",
    "                if previous_chunk[-(j+1):] == current_chunk[:(j+1)]:\n",
    "                    overlap_start = j + 1\n",
    "            \n",
    "            # Add the non-overlapping part of the current chunk\n",
    "            final_transcription.append(\" \".join(current_chunk[overlap_start:]))\n",
    "    \n",
    "    return \" \".join(final_transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_large_audio(input_file, whisper_api_key, max_size_mb=20, overlap_ms=1500, cleanup = False):\n",
    "    \"\"\"\n",
    "    Transcribes a large audio file by splitting it into smaller chunks with overlap, sending each to Whisper API,\n",
    "    and combining the results, removing overlap at chunk boundaries.\n",
    "    \n",
    "    :param input_file: Path to the input audio file (WAV).\n",
    "    :param whisper_api_key: OpenAI API key for Whisper API.\n",
    "    :param max_size_mb: Maximum size of each audio chunk in MB (25 MB for Whisper API).\n",
    "    :param overlap_ms: Overlap duration between consecutive chunks in milliseconds.\n",
    "    :return: Combined transcription text for the entire audio file.\n",
    "    \"\"\"\n",
    "    # Step 1: Split the audio into chunks with overlap\n",
    "    chunk_files = split_audio_by_size(input_file, max_size_mb, overlap_ms)\n",
    "\n",
    "    # Step 2: Transcribe each chunk using Whisper API\n",
    "    transcriptions = []\n",
    "    for chunk_file in chunk_files:\n",
    "        chunk_transcription = transcribe_audio_chunk(chunk_file, whisper_api_key)\n",
    "        if chunk_transcription:\n",
    "            transcriptions.append(chunk_transcription)\n",
    "    \n",
    "    # Step 3: Combine all transcriptions and remove overlap\n",
    "    full_transcription = remove_overlap_from_transcriptions(transcriptions)\n",
    "    \n",
    "    # Optionally, clean up chunk files\n",
    "    if cleanup:\n",
    "        for chunk_file in chunk_files:\n",
    "            os.remove(chunk_file)\n",
    "\n",
    "    return full_transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tim/Downloads/test.wav'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave = convert_audio_to_wav(\"/Users/tim/Downloads/test.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/Applications/miniconda3/envs/transcribe/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker SPEAKER_01 speaks from 0.0s to 47.3s\n",
      "Speaker SPEAKER_01 speaks from 47.8s to 57.3s\n",
      "Speaker SPEAKER_01 speaks from 59.2s to 84.9s\n",
      "Speaker SPEAKER_01 speaks from 86.0s to 86.4s\n",
      "Speaker SPEAKER_01 speaks from 87.7s to 96.6s\n",
      "Speaker SPEAKER_01 speaks from 98.3s to 112.2s\n",
      "Speaker SPEAKER_01 speaks from 112.4s to 117.0s\n",
      "Speaker SPEAKER_01 speaks from 117.5s to 120.8s\n",
      "Speaker SPEAKER_01 speaks from 121.0s to 121.5s\n",
      "Speaker SPEAKER_00 speaks from 122.3s to 123.0s\n",
      "Speaker SPEAKER_01 speaks from 123.0s to 123.0s\n",
      "Speaker SPEAKER_00 speaks from 123.0s to 126.4s\n",
      "Speaker SPEAKER_00 speaks from 128.0s to 130.7s\n",
      "Speaker SPEAKER_00 speaks from 132.0s to 133.2s\n",
      "Speaker SPEAKER_00 speaks from 134.8s to 135.0s\n",
      "Speaker SPEAKER_01 speaks from 135.0s to 135.3s\n",
      "Speaker SPEAKER_01 speaks from 137.7s to 141.6s\n",
      "Speaker SPEAKER_01 speaks from 143.9s to 144.4s\n",
      "Speaker SPEAKER_01 speaks from 146.0s to 147.4s\n",
      "Speaker SPEAKER_01 speaks from 148.6s to 150.8s\n",
      "Speaker SPEAKER_01 speaks from 151.9s to 156.4s\n",
      "Speaker SPEAKER_02 speaks from 156.9s to 162.1s\n",
      "Speaker SPEAKER_01 speaks from 162.3s to 164.5s\n",
      "Speaker SPEAKER_01 speaks from 166.4s to 166.8s\n",
      "Speaker SPEAKER_01 speaks from 168.0s to 169.7s\n",
      "Speaker SPEAKER_01 speaks from 170.5s to 171.0s\n",
      "Speaker SPEAKER_01 speaks from 172.1s to 187.1s\n",
      "Speaker SPEAKER_01 speaks from 188.9s to 190.9s\n",
      "Speaker SPEAKER_01 speaks from 191.2s to 191.6s\n",
      "Speaker SPEAKER_01 speaks from 192.5s to 195.1s\n",
      "Speaker SPEAKER_01 speaks from 195.5s to 197.2s\n",
      "Speaker SPEAKER_01 speaks from 198.4s to 200.4s\n",
      "Speaker SPEAKER_01 speaks from 200.8s to 202.2s\n",
      "Speaker SPEAKER_01 speaks from 203.8s to 205.7s\n",
      "Speaker SPEAKER_01 speaks from 207.9s to 208.4s\n",
      "Speaker SPEAKER_01 speaks from 215.9s to 216.7s\n",
      "Speaker SPEAKER_00 speaks from 223.6s to 230.3s\n",
      "Speaker SPEAKER_00 speaks from 231.6s to 233.2s\n",
      "Speaker SPEAKER_00 speaks from 233.7s to 237.2s\n",
      "Speaker SPEAKER_00 speaks from 237.5s to 244.5s\n",
      "Speaker SPEAKER_01 speaks from 245.2s to 250.4s\n",
      "Speaker SPEAKER_00 speaks from 250.9s to 253.2s\n",
      "Speaker SPEAKER_00 speaks from 253.4s to 255.1s\n",
      "Speaker SPEAKER_00 speaks from 257.6s to 261.9s\n",
      "Speaker SPEAKER_02 speaks from 260.9s to 261.8s\n",
      "Speaker SPEAKER_02 speaks from 261.9s to 268.9s\n",
      "Speaker SPEAKER_02 speaks from 269.6s to 269.6s\n",
      "Speaker SPEAKER_00 speaks from 269.6s to 270.0s\n",
      "Speaker SPEAKER_02 speaks from 270.0s to 270.0s\n",
      "Speaker SPEAKER_00 speaks from 270.0s to 270.5s\n",
      "Speaker SPEAKER_02 speaks from 270.5s to 270.6s\n",
      "Speaker SPEAKER_00 speaks from 270.6s to 274.8s\n",
      "Speaker SPEAKER_00 speaks from 275.2s to 283.0s\n",
      "Speaker SPEAKER_02 speaks from 278.4s to 279.0s\n",
      "Speaker SPEAKER_01 speaks from 279.9s to 279.9s\n",
      "Speaker SPEAKER_02 speaks from 279.9s to 281.8s\n",
      "Speaker SPEAKER_02 speaks from 282.0s to 282.1s\n",
      "Speaker SPEAKER_02 speaks from 283.0s to 288.8s\n",
      "Speaker SPEAKER_02 speaks from 290.0s to 291.1s\n",
      "Speaker SPEAKER_01 speaks from 297.4s to 302.4s\n",
      "Speaker SPEAKER_01 speaks from 302.8s to 305.0s\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "test=perform_diarization(\"/Users/tim/Downloads/test.wav\", hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(text=\"and holds it for a minute, concentrating. And as he does, it looks like he is expending a lot of energy. And the black, like little tendrils of black smoke begin to creep out of the crown and start to touch the mask and start to sort of billow into the mask. And then he turns and he shoves it on this person's face. And they briefly struggle for a minute, but not really. It's more like reflexes, not that there's some sort of conscious person who is fighting against it, but just the reflex of a body. If you were tied up and somebody was pouring water down your throat and you were choking, even if you didn't care, your reflexes would make you cough. And over the next three, four minutes, the mask sort of fuses into this person's face. And the shadow, the darkness envelops them. They begin to grow slightly. Out of their hand, a spear begins to grow and form. A cloak begins to form over their head and arms and draping down. They begin to become taller. The bonds fall away and they stand up. And Festo, then, hands, wearing a glove, picks up four chelite gems that also seem to be swirling. Chelite gems that also seem to be swirling in this shadow, hands them to this creature, along with a quickly sketched drawing of Kenzo, Seeker, and Delweth, but not Welby. Why not Welby? Because he never made, I don't know. Festo would know Welby. That's interesting. OK. Shows this to him. Actually, wait, who was? No, I guess Welby. You have a bunch of, you have your bow and your, you carry a bunch of magic items, right? The bow, the Raven's Whistle, the Message Stones, and the Driftglobe are what I have on me. So you would. I forgot about that.\")\n",
      "Transcription(text=\"So you would, I forgot about your bell, you would be in the drawing as well, and he says, he shows these four pictures to the creature and it takes the chain light shatter gems, puts them in his pocket, and sort of bows and then walks out through the wall of the prison. Through the wall? Yeah. No door. No door. Is this like, this isn't, this is some sort of, this isn't Faustus's workshop or anything like that, right? It's not the room you saw, but you didn't go to the top of the tower. Okay. Um. Interesting. Shadowy crown, bath pearls, probably one of the artifacts. It's one we don't know about, right? Yeah. What's probably one of the artifacts? That shadowy crown looked pretty, uh. So what was happening? There was like smoke coming out of it, and then it went into the mask, the mask on the guy, and the guy became like a zombie. Well, the guy looked like he was already pretty much a zombie, like his vacant expression. But the four pieces of chain light, he gave it to the guy? Yeah. And when he was referring to chain light zombies, was he referring to himself? Possibly. I think he's, I would assume he's referring to like the vacant eye, like the guy who got turned into the hunter, right? Yeah, but when we killed him, the mask fell off and he turned back into the guy who got turned into the hunter. Right, and he said there's so many more chain light zombies. There are a dime a dozen or something. So he's referring to himself. Yeah. I think so. Fuck. If it was he who was speaking at that point, rather than someone speaking through him. That's a good point. The one thing you have going for you is, at least in the mirror, it looked like this required a lot of power. Right.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transcription' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/tim/Downloads/test.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Transcribe the large audio file with overlap handling\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_large_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print the full transcription\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscription result:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, transcription)\n",
      "Cell \u001b[0;32mIn[41], line 23\u001b[0m, in \u001b[0;36mtranscribe_large_audio\u001b[0;34m(input_file, whisper_api_key, max_size_mb, overlap_ms, cleanup)\u001b[0m\n\u001b[1;32m     20\u001b[0m         transcriptions\u001b[38;5;241m.\u001b[39mappend(chunk_transcription)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Step 3: Combine all transcriptions and remove overlap\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m full_transcription \u001b[38;5;241m=\u001b[39m \u001b[43mremove_overlap_from_transcriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscriptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Optionally, clean up chunk files\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mremove_overlap_from_transcriptions\u001b[0;34m(transcriptions, overlap_word_count)\u001b[0m\n\u001b[1;32m     14\u001b[0m     final_transcription\u001b[38;5;241m.\u001b[39mappend(transcriptions[i])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Compare the last few words of the previous chunk with the start of the current chunk\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     previous_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_transcription\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[1;32m     18\u001b[0m     current_chunk \u001b[38;5;241m=\u001b[39m transcriptions[i]\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Find overlap by comparing the last few words of the previous chunk and the first few words of the current chunk\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/transcribe/lib/python3.12/site-packages/pydantic/main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transcription' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "whisper_api_key = os.getenv(\"OPEN_API_TAELGAR\")\n",
    "\n",
    "# Path to your WAV file\n",
    "input_file = \"/Users/tim/Downloads/test.wav\"\n",
    "\n",
    "# Transcribe the large audio file with overlap handling\n",
    "transcription = transcribe_large_audio(input_file, whisper_api_key, max_size_mb=15, overlap_ms=2000)\n",
    "\n",
    "# Print the full transcription\n",
    "print(\"Transcription result:\\n\", transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
