{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Mismatch Diagnostics\n",
    "\n",
    "This notebook helps compare Zoom-trained speaker embeddings against iPhone (or other domain) clips. It loads the trained classifier bundle, extracts embeddings for both Zoom clips and test clips, and runs a cosine-similarity sanity check alongside the existing sklearn head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from train_speaker_classifier import FeatureExtractor, resolve_hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure paths\n",
    "\n",
    "Update the paths below:\n",
    "- `MODEL_PATH`: trained speaker model bundle (`speaker_classifier.joblib`).\n",
    "- `ZOOM_MANIFEST`: manifest JSONL produced by `generate_speaker_corpus.py` (Zoom domain clips).\n",
    "- `IPHONE_DIR`: directory of test clips (e.g., iPhone chunks) organized as `speaker_name/*.wav` or just a bag of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models/speaker_ecapa/speaker_classifier.joblib\")\n",
    "ZOOM_MANIFEST = Path(\"/path/to/zoom/manifest.jsonl\")\n",
    "IPHONE_DIR = Path(\"/path/to/iphone/clips\")\n",
    "\n",
    "# Optional override if the model requires a gated HF token (pyannote/embedding)\n \\nHF_TOKEN = resolve_hf_token(None)\n",
    "assert MODEL_PATH.exists(), MODEL_PATH\n",
    "assert ZOOM_MANIFEST.exists(), ZOOM_MANIFEST\n",
    "assert IPHONE_DIR.exists(), IPHONE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model + feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = joblib.load(MODEL_PATH)\n",
    "model = bundle[\"model\"]\n",
    "label_encoder = bundle[\"label_encoder\"]\n",
    "feature_params = bundle.get(\"feature_params\", {})\n",
    "sample_rate = int(feature_params.get(\"sample_rate\", 16_000))\n",
    "n_mfcc = int(feature_params.get(\"n_mfcc\", 40))\n",
    "feature_type = feature_params.get(\"feature_type\", \"mfcc\")\n",
    "wav2vec2_model = feature_params.get(\"wav2vec2_model\")\n",
    "ecapa_model = feature_params.get(\"ecapa_model\")\n",
    "pyannote_model = feature_params.get(\"pyannote_model\")\n",
    "\n",
    "extractor = FeatureExtractor(\n",
    "    feature_type=feature_type,\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=n_mfcc,\n",
    "    wav2vec2_model=wav2vec2_model,\n",
    "    ecapa_model=ecapa_model,\n",
    "    pyannote_model=pyannote_model,\n",
    "    hf_token=HF_TOKEN,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "feature_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manifest_entries(manifest_path: Path) -> list[dict]:\n",
    "    entries = []\n",
    "    with manifest_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            entries.append(data)\n",
    "    return entries\n",
    "\n",
    "def compute_embedding(path: Path) -> np.ndarray:\n",
    "    audio, _ = librosa.load(path, sr=sample_rate, mono=True)\n",
    "    return extractor.compute_from_waveform(audio, sample_rate)\n",
    "\n",
    "def l2_normalize(vec: np.ndarray) -> np.ndarray:\n",
    "    norm = np.linalg.norm(vec) + 1e-9\n",
    "    return vec / norm\n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Zoom centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_entries = load_manifest_entries(ZOOM_MANIFEST)\n",
    "zoom_embeds: dict[str, list[np.ndarray]] = defaultdict(list)\n",
    "for entry in tqdm(zoom_entries, desc=\"Zoom embeddings\"):\n",
    "    clip_path = Path(entry[\"clip_path\"]).expanduser()\n",
    "    if not clip_path.exists():\n",
    "        continue\n",
    "    try:\n",
    "        emb = compute_embedding(clip_path)\n",
    "    except Exception as exc:\n",
    "        print(f\"[warn] {clip_path}: {exc}\")\n",
    "        continue\n",
    "    zoom_embeds[entry[\"speaker\"]].append(emb)\n",
    "\n",
    "zoom_centroids = {\n",
    "    speaker: l2_normalize(np.mean(np.stack(embs), axis=0))\n",
    "    for speaker, embs in zoom_embeds.items()\n",
    "    if embs\n",
    "}\n",
    "list(zoom_centroids.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed iPhone / target-domain clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_records = []\n",
    "for path in sorted(IPHONE_DIR.rglob(\"*.wav\")):\n",
    "    speaker_guess = path.parent.name\n",
    "    try:\n",
    "        emb = compute_embedding(path)\n",
    "    except Exception as exc:\n",
    "        print(f\"[warn] {path}: {exc}\")\n",
    "        continue\n",
    "    iphone_records.append({\n",
    "        \"path\": path,\n",
    "        \"speaker_guess\": speaker_guess,\n",
    "        \"embedding\": emb,\n",
    "    })\n",
    "len(iphone_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_centroid(embedding: np.ndarray) -> tuple[str, float, dict[str, float]]:\n",
    "    emb = l2_normalize(embedding)\n",
    "    scores = {}\n",
    "    best_spk, best_score = None, -1.0\n",
    "    for speaker, centroid in zoom_centroids.items():\n",
    "        score = cosine(emb, centroid)\n",
    "        scores[speaker] = score\n",
    "        if score > best_score:\n",
    "            best_spk, best_score = speaker, score\n",
    "    return best_spk, best_score, scores\n",
    "\n",
    "rows = []\n",
    "for record in iphone_records:\n",
    "    prediction, score, scores = predict_by_centroid(record[\"embedding\"])\n",
    "    rows.append({\n",
    "        \"path\": str(record[\"path\"]),\n",
    "        \"speaker_guess\": record[\"speaker_guess\"],\n",
    "        \"centroid_prediction\": prediction,\n",
    "        \"centroid_score\": score,\n",
    "    })\n",
    "cosine_df = pd.DataFrame(rows)\n",
    "cosine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect cosine ranking per clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_matches(record, top_n=5):\n",
    "    _, _, scores = predict_by_centroid(record[\"embedding\"])\n",
    "    ordered = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "    print(f\"Clip: {record['path']}\")\n",
    "    print(f\"  Labeled speaker: {record['speaker_guess']}\")\n",
    "    for spk, score in ordered:\n",
    "        print(f\"  {spk:20s}  score={score:.3f}\")\n",
    "\n",
    "# Example: inspect the first few\n",
    "for record in iphone_records[:3]:\n",
    "    show_top_matches(record)\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare classifier predictions vs cosine baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rows = []\n",
    "for record in iphone_records:\n",
    "    vec = record[\"embedding\"].reshape(1, -1)\n",
    "    pred = model.predict(vec)[0]\n",
    "    classifier_rows.append({\n",
    "        \"path\": str(record[\"path\"]),\n",
    "        \"speaker_guess\": record[\"speaker_guess\"],\n",
    "        \"classifier_prediction\": label_encoder.inverse_transform([pred])[0],\n",
    "    })\n",
    "\n",
    "classifier_df = pd.DataFrame(classifier_rows)\n",
    "comparison_df = cosine_df.merge(classifier_df, on=[\"path\", \"speaker_guess\"], how=\"left\")\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now:\n",
    "- Filter `comparison_df` where centroid prediction matches the known speaker but classifier does not (diagnose scaler/head issues).\n",
    "- Inspect raw cosine scores to see if the correct speaker is usually top-ranked.\n",
    "- Plot distributions of cosine scores vs classifier confidence to understand the domain gap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
