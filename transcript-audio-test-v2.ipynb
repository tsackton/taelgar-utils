{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/Applications/miniconda3/envs/transcribe/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_wav(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Converts an audio file of any supported format to WAV format using pydub, with error checking.\n",
    "    \n",
    "    :param input_file: Path to the input audio file (e.g., m4a, mp3, wav, ogg).\n",
    "    :param output_file: Optional, path to save the converted wav file. \n",
    "                        If not provided, the output file will have the same name as the input but with a .wav extension.\n",
    "    :return: Path to the converted WAV file or None if conversion fails.\n",
    "    \"\"\"\n",
    "    # Check if ffmpeg is installed and available\n",
    "    if not shutil.which(\"ffmpeg\"):\n",
    "        raise EnvironmentError(\"ffmpeg is not installed or not found in system PATH. Please install ffmpeg.\")\n",
    "\n",
    "    # Detect file format based on extension\n",
    "    file_format = input_file.split('.')[-1].lower()\n",
    "\n",
    "    # Supported formats by pydub + ffmpeg\n",
    "    supported_formats = ['m4a', 'mp3', 'ogg', 'flv', 'aac', 'wma', 'flac', 'wav']\n",
    "\n",
    "    # Check if the file format is supported\n",
    "    if file_format not in supported_formats:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}. Supported formats are: {supported_formats}\")\n",
    "\n",
    "    try:\n",
    "        # Try to load the audio file\n",
    "        audio = AudioSegment.from_file(input_file, format=file_format)\n",
    "\n",
    "        # If output file is not provided, generate the output file name\n",
    "        if output_file is None:\n",
    "            output_file = input_file.rsplit('.', 1)[0] + '.wav'\n",
    "\n",
    "        # Export the audio file as wav\n",
    "        audio.export(output_file, format=\"wav\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The input file '{input_file}' does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting the audio file: {e}\")\n",
    "        return None\n",
    "\n",
    "def perform_diarization(wav_file, hf_token, num_speakers=None):\n",
    "    \"\"\"\n",
    "    Perform speaker diarization on a WAV audio file using pyannote.audio, with Hugging Face token for authentication.\n",
    "    Utilizes MPS backend on Mac M1/M2 if available.\n",
    "    \n",
    "    :param wav_file: Path to the WAV file.\n",
    "    :param hf_token: Hugging Face token for accessing the pre-trained model.\n",
    "    :return: Diarization object with speaker segments and timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the speaker diarization pipeline using the provided Hugging Face token\n",
    "\n",
    "        pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=hf_token)\n",
    "\n",
    "        # Check if MPS is available, otherwise default to CPU\n",
    "        if torch.backends.mps.is_available():\n",
    "            pipeline.to(torch.device(\"mps\")) \n",
    "\n",
    "         # Perform speaker diarization, specifying the number of speakers if provided\n",
    "        if num_speakers:\n",
    "            diarization = pipeline(wav_file, num_speakers=num_speakers)\n",
    "        else:\n",
    "            diarization = pipeline(wav_file)\n",
    "\n",
    "        # Collect the results in a structured format\n",
    "        results = []\n",
    "        for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            result = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": segment.start,\n",
    "                \"end_time\": segment.end\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"Speaker {speaker} speaks from {segment.start:.1f}s to {segment.end:.1f}s\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during diarization: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Function to split audio into chunks\n",
    "def split_audio_by_size(input_file, max_size_mb=20, overlap_ms=1000):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "    bytes_per_second = audio.frame_rate * audio.frame_width * audio.channels\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    chunk_duration_ms = math.floor((max_size_bytes / bytes_per_second) * 1000)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(audio):\n",
    "        end = start + chunk_duration_ms\n",
    "        chunks.append((audio[start:end], start))\n",
    "        start = end - overlap_ms\n",
    "    \n",
    "    chunk_files = []\n",
    "    for i, (chunk, chunk_start_time_ms) in enumerate(chunks):\n",
    "        chunk_filename = f\"{os.path.splitext(input_file)[0]}_chunk_{i}.wav\"\n",
    "        chunk.export(chunk_filename, format=\"wav\")\n",
    "        chunk_files.append((chunk_filename, chunk_start_time_ms))\n",
    "    \n",
    "    return chunk_files\n",
    "\n",
    "# Function to transcribe a single chunk and adjust timestamps\n",
    "def transcribe_audio_chunk(chunk_file, whisper_api_key, chunk_start_time_ms, debug=False):   \n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=whisper_api_key)\n",
    "\n",
    "    try:\n",
    "        with open(chunk_file, \"rb\") as audio_file:\n",
    "            response = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", \n",
    "                file=audio_file, \n",
    "                response_format=\"verbose_json\",\n",
    "                timestamp_granularities=[\"word\"]\n",
    "            )\n",
    "            \n",
    "            words = response.words\n",
    "            print(response)\n",
    "            print(words)\n",
    "            adjusted_words = []\n",
    "            for word in words:\n",
    "                adjusted_words.append({\n",
    "                    'word': word.word,\n",
    "                    'start': word.start + chunk_start_time_ms / 1000,\n",
    "                    'end': word.start + chunk_start_time_ms / 1000\n",
    "                })\n",
    "\n",
    "            print(adjusted_words)\n",
    "            return adjusted_words\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to synchronize diarization results with transcriptions\n",
    "def synchronize_diarization_with_whisper(diarization_results, transcribed_chunks):\n",
    "    synchronized_output = []\n",
    "    words = [word for chunk in transcribed_chunks for word in chunk]\n",
    "    \n",
    "    for segment in diarization_results:\n",
    "        segment_start = segment['start_time']\n",
    "        segment_end = segment['end_time']\n",
    "        speaker = segment['speaker']\n",
    "        \n",
    "        segment_words = [\n",
    "            word['word'] for word in words \n",
    "            if word['start'] >= segment_start and word['end'] <= segment_end\n",
    "        ]\n",
    "        segment_text = \" \".join(segment_words)\n",
    "        \n",
    "        synchronized_output.append({\n",
    "            'speaker': speaker,\n",
    "            'text': segment_text,\n",
    "            'start_time': segment_start,\n",
    "            'end_time': segment_end\n",
    "        })\n",
    "    \n",
    "    return synchronized_output\n",
    "\n",
    "# Main function to split, transcribe, and synchronize\n",
    "def transcribe_and_sync(input_file, whisper_api_key, diarization_results, max_size_mb=20, overlap_ms=1000, debug=False):\n",
    "    chunk_files = split_audio_by_size(input_file, max_size_mb, overlap_ms)\n",
    "    transcribed_chunks = []\n",
    "    \n",
    "    for chunk_file, chunk_start_time_ms in chunk_files:\n",
    "        chunk_transcription = transcribe_audio_chunk(chunk_file, whisper_api_key, chunk_start_time_ms, debug=debug)\n",
    "        print(f\"Transcribing {chunk_file}\")\n",
    "        if chunk_transcription:\n",
    "            transcribed_chunks.append(chunk_transcription)\n",
    "    \n",
    "    synchronized_transcription = synchronize_diarization_with_whisper(diarization_results, transcribed_chunks)\n",
    "\n",
    "    for chunk_file, _ in chunk_files:\n",
    "        os.remove(chunk_file)\n",
    "\n",
    "    return synchronized_transcription\n",
    "\n",
    "def print_clean_transcript(synchronized_transcription, wrap_width=None):\n",
    "    \"\"\"\n",
    "    Prints a clean transcript with speakers and their corresponding text, combining consecutive speech from the same speaker.\n",
    "    Optionally wraps text to a specified width.\n",
    "    \n",
    "    :param synchronized_transcription: List of synchronized speaker and text segments.\n",
    "    :param wrap_width: Optional width for word wrapping the text. If None, no wrapping is applied.\n",
    "    \"\"\"\n",
    "    previous_speaker = None\n",
    "    combined_text = \"\"\n",
    "\n",
    "    for segment in synchronized_transcription:\n",
    "        current_speaker = segment['speaker']\n",
    "        current_text = segment['text'].strip()  # Strip leading/trailing whitespace\n",
    "\n",
    "        if not current_text:\n",
    "            # Skip empty or whitespace-only segments\n",
    "            continue\n",
    "\n",
    "        if current_speaker == previous_speaker:\n",
    "            # Continue combining text if the speaker is the same\n",
    "            combined_text += \" \" + current_text\n",
    "        else:\n",
    "            # Print the previous speaker's text before starting a new speaker\n",
    "            if previous_speaker is not None:\n",
    "                # Apply word wrapping if wrap_width is provided\n",
    "                if wrap_width:\n",
    "                    wrapped_text = textwrap.fill(combined_text.strip(), width=wrap_width, subsequent_indent='  ')\n",
    "                else:\n",
    "                    wrapped_text = combined_text.strip()\n",
    "                print(f\"{previous_speaker}: {wrapped_text}\")\n",
    "            \n",
    "            # Start a new speaker block\n",
    "            previous_speaker = current_speaker\n",
    "            combined_text = current_text\n",
    "\n",
    "    # Print the last speaker's text after the loop ends\n",
    "    if previous_speaker is not None:\n",
    "        # Apply word wrapping if wrap_width is provided\n",
    "        if wrap_width:\n",
    "            wrapped_text = textwrap.fill(combined_text.strip(), width=wrap_width, subsequent_indent='  ')\n",
    "        else:\n",
    "            wrapped_text = combined_text.strip()\n",
    "        print(f\"{previous_speaker}: {wrapped_text}\")\n",
    "\n",
    "def write_clean_transcript_to_file(synchronized_transcription, file_path, wrap_width=None):\n",
    "    \"\"\"\n",
    "    Writes a clean transcript with speakers and their corresponding text, combining consecutive speech from the same speaker.\n",
    "    Optionally wraps text to a specified width.\n",
    "    \n",
    "    :param synchronized_transcription: List of synchronized speaker and text segments.\n",
    "    :param file_path: Path to the file where the cleaned transcript will be written.\n",
    "    :param wrap_width: Optional width for word wrapping the text. If None, no wrapping is applied.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        previous_speaker = None\n",
    "        combined_text = \"\"\n",
    "\n",
    "        for segment in synchronized_transcription:\n",
    "            current_speaker = segment['speaker']\n",
    "            current_text = segment['text'].strip()  # Strip leading/trailing whitespace\n",
    "\n",
    "            if not current_text:\n",
    "                # Skip empty or whitespace-only segments\n",
    "                continue\n",
    "\n",
    "            if current_speaker == previous_speaker:\n",
    "                # Continue combining text if the speaker is the same\n",
    "                combined_text += \" \" + current_text\n",
    "            else:\n",
    "                # Write the previous speaker's text before starting a new speaker\n",
    "                if previous_speaker is not None:\n",
    "                    # Apply word wrapping if wrap_width is provided\n",
    "                    if wrap_width:\n",
    "                        wrapped_text = textwrap.fill(combined_text.strip(), width=wrap_width, subsequent_indent='  ')\n",
    "                    else:\n",
    "                        wrapped_text = combined_text.strip()\n",
    "                    file.write(f\"{previous_speaker}: {wrapped_text}\\n\")\n",
    "                \n",
    "                # Start a new speaker block\n",
    "                previous_speaker = current_speaker\n",
    "                combined_text = current_text\n",
    "\n",
    "        # Write the last speaker's text after the loop ends\n",
    "        if previous_speaker is not None:\n",
    "            if wrap_width:\n",
    "                wrapped_text = textwrap.fill(combined_text.strip(), width=wrap_width, subsequent_indent='  ')\n",
    "            else:\n",
    "                wrapped_text = combined_text.strip()\n",
    "            file.write(f\"{previous_speaker}: {wrapped_text}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "whisper_api_key = os.getenv(\"OPEN_API_TAELGAR\")\n",
    "\n",
    "input_file = convert_audio_to_wav(\"/Users/tim/Downloads/Dunmar-CO-Rec1.m4a\")\n",
    "prefix = \"/Users/tim/Downloads/dunmar-co-segment1\"\n",
    "output_file = f\"{prefix}.transcript.txt\"\n",
    "num_speakers = 5\n",
    "\n",
    "diarization_results = perform_diarization(input_file, hf_token, num_speakers=num_speakers)\n",
    "print(\"Finished diarization.\")\n",
    "synchronized_transcription = transcribe_and_sync(input_file, whisper_api_key, diarization_results, debug=False)  \n",
    "write_clean_transcript_to_file(synchronized_transcription, output_file, wrap_width=None)\n",
    "pickle.dump(synchronized_transcription, open(f\"{prefix}.transcript.pkl\", \"wb\"))\n",
    "pickle.dump(diarization_results, open(f\"{prefix}.diarization.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
